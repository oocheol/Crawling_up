{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모두 실행하면 자동으로 크롤링\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from Screenshot import Screenshot_Clipping\n",
    "import random\n",
    "import pyperclip\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless') #내부 창을 띄울 수 없으므로 설정\n",
    "chrome_options.add_argument('--no-sandbox') #sandbox 사용 X\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "chrome_path = 'C:/chromedriver/chromedriver.exe'\n",
    "driver = webdriver.Chrome(chrome_path, chrome_options=chrome_options)\n",
    "# driver1 = webdriver.Chrome(chrome_path, chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 입력 안될 시 copy 함수 사용\n",
    "\n",
    "def copy_input(xpath, input):\n",
    "    pyperclip.copy(input)\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "    ActionChains(driver).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "email = '아이디'\n",
    "pw = '비밀번호'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawling_Amazon(input_num, remain_num=22):\n",
    "       \n",
    "        url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "\n",
    "        driver.get(url)\n",
    "        try :\n",
    "        # 아이디 로그인\n",
    "                driver.find_element_by_xpath('//*[@id=\"ap_email\"]').send_keys('아이디')\n",
    "                driver.find_element_by_xpath('//*[@id=\"ap_password\"]').send_keys('비밀번호')\n",
    "                time.sleep(random.randint(3,5))\n",
    "                driver.find_element_by_xpath('//*[@id=\"signInSubmit\"]').click()\n",
    "\n",
    "                time.sleep(random.randint(3,5))\n",
    "\n",
    "                url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "                driver.get(url)\n",
    "\n",
    "                time.sleep(random.randint(3,5))\n",
    "               \n",
    "        except :\n",
    "                url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "                driver.get(url)\n",
    "                time.sleep(random.randint(3,5))\n",
    "\n",
    "\n",
    "        for i in range(2, remain_num):\n",
    "                # 생성 시간 (추후 index 값으로 설정) li 2 ~ 21\n",
    "                created_time = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[1]/span[5]/span')\n",
    "                time.sleep(random.randint(3,5))\n",
    "                created_time.click()\n",
    "                created_time = created_time.get_attribute('title')\n",
    "\n",
    "                # 제목\n",
    "                title = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[1]/span[2]').text\n",
    "\n",
    "                # 신청인\n",
    "                requester = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[1]/span[1]')\n",
    "                requester = requester.get_attribute('title')\n",
    "\n",
    "                # 리워드\n",
    "                reward = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[1]/span[4]').text\n",
    "\n",
    "                # HITs (목표 수량)\n",
    "                hits = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[1]/span[3]').text\n",
    "\n",
    "                try :\n",
    "                        # 할당 시간 (Min 단위 숫자만 추출)\n",
    "                        time_allotted = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div/div/div[2]/div/div[3]').text\n",
    "                        time_allotted = time_allotted[time_allotted.index('Allotted')+9 : time_allotted.index('Min')-1]\n",
    "\n",
    "                except :\n",
    "                        # 할당 시간 (Min 단위 숫자만 추출)\n",
    "                        time_allotted = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div/div/div[2]/div/div[3]').text\n",
    "                        time_allotted = time_allotted[time_allotted.index('Allotted')+9 : time_allotted.index('Sec')-1]\n",
    "\n",
    "                # 마감 날짜\n",
    "                expires = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div/div/div[2]/div/div[4]/span')\n",
    "                expires = expires.get_attribute('title')\n",
    "\n",
    "                # 상세설명\n",
    "                explain = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div/div/div[1]/p').text\n",
    "\n",
    "                try :\n",
    "                        # 조건 (변수명 : limit_숫자)\n",
    "\n",
    "                        for j in range(1,10):\n",
    "                                try:\n",
    "                                        globals()[f'limit_{j}'] = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div[2]/div[2]/div[{j}]/div[1]/span').text\n",
    "                                       \n",
    "                                except:\n",
    "                                        for k in range(1, j):\n",
    "                                                # limit 1열\n",
    "                                                limit_col1 = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div[2]/div[2]/div[{k}]/div[1]/span').text\n",
    "                                                # limit 2열\n",
    "                                                limit_col2 = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div[2]/div[2]/div[{k}]/div[2]').text\n",
    "                                                # limit 3열\n",
    "                                                limit_col3 = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[3]/div/div[2]/div[2]/div[{k}]/div[3]').text\n",
    "\n",
    "                                                globals()[f'limit_{k}'] = [[limit_col1], [limit_col2], [limit_col3]]\n",
    "                                                lim = j\n",
    "                                        break\n",
    "                       \n",
    "                        lim_list = f'''{[globals()[f'limit_{s}'] for s in range(1, lim)]}'''\n",
    "                       \n",
    "                        Amazon_M_turk = {'created_time': created_time, 'title': title, 'requester': requester, 'hits':hits, 'reward':reward, 'time_allotted(Min)':time_allotted, 'expires':expires, 'explain':explain, 'limit': lim_list}\n",
    "                                               \n",
    "                        Amazon_M_turk_df = pd.DataFrame(Amazon_M_turk, index=[0])\n",
    "                        Amazon_M_turk_df.set_index('created_time', inplace=True)\n",
    "                        globals()[f'Amazon_M_turk_df_{i}'] = Amazon_M_turk_df\n",
    "\n",
    "                except :        \n",
    "                        Amazon_M_turk = {'created_time': created_time, 'title': title, 'requester': requester, 'hits':hits, 'reward':reward, 'time_allotted(Min)':time_allotted, 'expires':expires, 'explain':explain }\n",
    "                        Amazon_M_turk_df = pd.DataFrame(Amazon_M_turk, index=[0])\n",
    "                        Amazon_M_turk_df.set_index('created_time', inplace=True)\n",
    "                        globals()[f'Amazon_M_turk_df_{i}'] = Amazon_M_turk_df\n",
    "\n",
    "        df = pd.concat([globals()[f'Amazon_M_turk_df_{c}'] for c in range(2,remain_num)])\n",
    "\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawling_Amazon_review(input_num, remain_num=23):\n",
    "        i = 2\n",
    "        url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "\n",
    "        driver.get(url)\n",
    "        try :\n",
    "        # 아이디 로그인\n",
    "                copy_input('//*[@id=\"ap_email\"]', email)\n",
    "                time.sleep(random.randint(3,5))\n",
    "                copy_input('//*[@id=\"ap_password\"]', pw)\n",
    "                time.sleep(random.randint(3,5))\n",
    "                driver.find_element_by_xpath('//*[@id=\"signInSubmit\"]').click()\n",
    "\n",
    "\n",
    "                time.sleep(random.randint(5,10))\n",
    "\n",
    "                url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "                driver.get(url)\n",
    "\n",
    "                time.sleep(random.randint(10,15))\n",
    "                \n",
    "        except :\n",
    "                url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "                driver.get(url)\n",
    "                time.sleep(random.randint(3,5))\n",
    "        \n",
    "\n",
    "        while i < remain_num :\n",
    "                try :\n",
    "                        driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[4]/div/div/ol/li[{i}]/div[1]/span[6]/span/a').click()\n",
    "                        time.sleep(random.randint(3,5))\n",
    "                \n",
    "                        # 생성 시간\n",
    "                        created_time = driver.find_element_by_xpath('/html/body/div[2]/div[2]/div/div[2]/div/div[4]/span[3]/span/span')\n",
    "                        created_time = created_time.get_attribute('title')\n",
    "                        created_time_name = created_time.replace('/', '.')\n",
    "                        created_time_name = created_time_name.replace(':', '.')\n",
    "\n",
    "                        # 타이틀\n",
    "                        title = driver.find_element_by_xpath('/html/body/div[2]/div[2]/div/div[1]/div/div/div/div[1]')\n",
    "                        title = title.get_attribute('title')\n",
    "\n",
    "                        iframe = driver.find_element_by_xpath(f'//*[@id=\"MainContent\"]/div[5]/div/div/iframe')\n",
    "                        url = iframe.get_attribute('src')\n",
    "                        driver.get(url)\n",
    "                        time.sleep(random.randint(3,5))\n",
    "\n",
    "                        # review_text\n",
    "                        review_text = driver.find_element_by_xpath('/html/body').text\n",
    "\n",
    "                        ob = Screenshot_Clipping.Screenshot()\n",
    "                        time.sleep(random.randint(3,5))\n",
    "                        ob.full_Screenshot(driver, save_path=r'./Amazon_img/', image_name=f'Amazon_{created_time_name}_{title}.png')\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        Amazon_M_turk = {'created_time': created_time, 'title': title, 'review_text': review_text}\n",
    "                        Amazon_M_turk_df = pd.DataFrame(Amazon_M_turk, index=[0])\n",
    "                        Amazon_M_turk_df.set_index('created_time', inplace=True)\n",
    "                        globals()[f'Amazon_M_turk_df_{i}'] = Amazon_M_turk_df\n",
    "                        \n",
    "                        url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "                        driver.get(url)\n",
    "                        time.sleep(random.randint(3,5))\n",
    "                        i += 1\n",
    "                        \n",
    "                except :\n",
    "                        Amazon_M_turk = {'created_time': '11/2/2021 02:22pm', 'title': 'Classify Receipt'}\n",
    "                        Amazon_M_turk_df = pd.DataFrame(Amazon_M_turk, index=[0])\n",
    "                        Amazon_M_turk_df.set_index('created_time', inplace=True)\n",
    "                        globals()[f'Amazon_M_turk_df_{i}'] = Amazon_M_turk_df\n",
    "                        \n",
    "                        url = f'https://worker.mturk.com/?sort=updated_desc&page_number={input_num}'\n",
    "                        driver.get(url)\n",
    "                        time.sleep(random.randint(3,5))\n",
    "                        i += 1\n",
    "        \n",
    "\n",
    "        df = pd.concat([globals()[f'Amazon_M_turk_df_{c}'] for c in range(2, remain_num)])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = pd.concat([Crawling_Amazon(x, 22) for x in range(30, 1, -1)])\n",
    "# df_list = df_list.reset_index()\n",
    "# df_list = df_list.drop_duplicates(['created_time', 'title'])\n",
    "# df_list.set_index('created_time', inplace=True)\n",
    "\n",
    "# df_list.to_csv('C:/Users/ooche/OneDrive - AROB/df_list.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review = pd.concat([Crawling_Amazon_review(x, 23) for x in range(30, 1, -1)])\n",
    "# df_review = df_review.reset_index()\n",
    "# df_review = df_review.drop_duplicates(['created_time', 'title'])\n",
    "# df_review.set_index('created_time', inplace=True)\n",
    "\n",
    "# df_review.to_csv('C:/Users/ooche/OneDrive - AROB/df_review.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merge = pd.merge(df_list, df_review, on = ['created_time', 'title'], how='outer')\n",
    "# df_merge.set_index('created_time', inplace=True)\n",
    "# df_merge.to_csv('C:/Users/ooche/OneDrive - AROB/df_merge.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5 = pd.read_csv('df5.csv')\n",
    "# df5 = df5.reset_index()\n",
    "\n",
    "while True:\n",
    "    df5 = pd.read_csv('C:/Users/ooche/OneDrive - AROB/df_list.csv') # 기존 파일 불러오기\n",
    "    df4 = Crawling_Amazon(1, 22) # 새로 크롤링한 파일\n",
    "    df4 = df4.reset_index()\n",
    "    df5 = pd.concat([df4, df5], ignore_index=False) # 새로 갱신된 파일과 merge\n",
    "    df5 = df5.drop_duplicates(['created_time', 'title']) # 중복 제거\n",
    "    df5 = df5.reset_index(drop=True)\n",
    "    df5.set_index('created_time', inplace=True)\n",
    "\n",
    "    df_review = pd.read_csv('C:/Users/ooche/OneDrive - AROB/df_review.csv') # 기존 파일 불러오기\n",
    "    df10 = Crawling_Amazon_review(1, 23) # 새로 크롤링한 파일\n",
    "    df10 = df10.reset_index()\n",
    "    df10 = pd.concat([df10, df_review], ignore_index=False) # 새로 갱신된 파일과 merge\n",
    "    df10 = df10.drop_duplicates(['created_time', 'title']) # 중복 제거\n",
    "    df10 = df10.reset_index(drop=True)\n",
    "    df10.set_index('created_time', inplace=True)\n",
    "\n",
    "    df10.to_csv('C:/Users/ooche/OneDrive - AROB/df_review.csv', encoding='utf-8-sig') # 저장\n",
    "\n",
    "    df5.to_csv('C:/Users/ooche/OneDrive - AROB/df_list.csv', encoding='utf-8-sig') # 저장\n",
    "\n",
    "    df_list = pd.read_csv('C:/Users/ooche/OneDrive - AROB/df_list.csv') # 통합된 list csv 불러오기\n",
    "    df_review = pd.read_csv('C:/Users/ooche/OneDrive - AROB/df_review.csv') # 통합된 review csv 불러오기\n",
    "    df_review = df_review.drop('created_time', axis=1)\n",
    "    df_merge = pd.merge(df_list, df_review, on = 'title', how='left') # list 기준 merge\n",
    "    df_merge = df_merge.drop_duplicates(['created_time', 'title']) # 중복제거\n",
    "    df_merge.set_index('created_time', inplace=True)\n",
    "    df_merge.to_csv('C:/Users/ooche/OneDrive - AROB/df_merge_ing.csv', encoding='utf-8-sig') # 통합본 저장\n",
    "\n",
    "    time.sleep(random.randint(3500,3700))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5327134b4d9c90ef246a62c4b08a4aaf90b0cea1e5a2b066b88c3413ddfa8b04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dacon': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
